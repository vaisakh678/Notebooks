{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"news2.txt\").read().replace(\"\\n\\n\", \" \").replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tknize(txt):\n",
    "    tokenize_sentences = []\n",
    "    tokenize_sentence_raw = sent_tokenize(txt)\n",
    "    for sentence in tokenize_sentence_raw:\n",
    "        tokenize_words = []\n",
    "        for word in word_tokenize(sentence):\n",
    "            tokenize_words.append(word)\n",
    "        tokenize_sentences.append(tokenize_words)\n",
    "    return tokenize_sentences, tokenize_sentence_raw\n",
    "\n",
    "def removeStopWords(tokenize_text):\n",
    "    englishStopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    tknize_sentence = []\n",
    "    for sentence in tokenize_text:\n",
    "        tknize_words = []\n",
    "        for word in sentence:\n",
    "            if word not in englishStopwords: tknize_words.append(word)\n",
    "        tknize_sentence.append(tknize_words)\n",
    "    return tknize_sentence\n",
    "\n",
    "def removePunch(tokenize_text):\n",
    "    tokenize_sentence = []\n",
    "    hash = set(\"\"\"!@#$%^&*(),./<<>?;\"';:}{`~\\|[]\"\"\")\n",
    "    for sentence in tokenize_text:\n",
    "        tokenize_words = []\n",
    "        for word in sentence:\n",
    "            wrd = \"\"\n",
    "            for chr in word:\n",
    "                if chr not in hash:\n",
    "                    wrd+=chr\n",
    "            if len(wrd) > 0:\n",
    "                tokenize_words.append(wrd)\n",
    "        tokenize_sentence.append(tokenize_words)\n",
    "    return tokenize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_sentences, tokenize_sentence_raw = tknize(text)\n",
    "tokenize_text = removeStopWords(tokenize_sentences)\n",
    "tokenize_text = removePunch(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = Word2Vec(tokenize_text, min_count=1, vector_size=100, window=3, sg=1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'film', 'The', 'Maverick', 'Cruise', 's', 'happy', 'Gun', 'best', 'first', 'also', 'It', 'A', 'said', 'Top', 'showed', 'opening', 'ever', 'Tom', 'American', 'broken', 'Day', 'Memorial', 'Drive', 'record', 'broke', 'new', 'time', 'release', 'widest', 'Canada', 'theatres', 'USA', 'sales', 'cinemas', '4700', 'records', 'Over', 'biggest', 'history', 'number', 'North', 'He', 'Depp', 'beat', 'called', 'I', 'm', 'filmmakers', 'great', 'reviews', 'UK', 'newspaper', 'thrilling', 'over-the-top', 'blockbuster', 'Another', 'studio', 'action', 'years', 'At', 'want', 'fantastic', 'ridiculously', 'Johnny', 'weekend', 'make', '2007', 'Pirates', 'Caribbean', 'gave', '100', 'million', 'sequel', 'results', '1986', 'production', 'company', 'done', 'well', 'spokesman', 'These', 'My']\n"
     ]
    }
   ],
   "source": [
    "words = list(vec.wv.index_to_key)\n",
    "print(words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>sentence vector</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceVector = []\n",
    "for sentence in tokenize_text:\n",
    "    sum = 0\n",
    "    for word in sentence:\n",
    "        sum+=vec.wv[word]\n",
    "    sum/=len(sentence)\n",
    "    sentenceVector.append(sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceCosinSimilarity = {}\n",
    "for i in range(len(sentenceVector)):\n",
    "    for j in range(len(sentenceVector)):\n",
    "        if i in sentenceCosinSimilarity:\n",
    "            sentenceCosinSimilarity[i] += cosine_similarity([sentenceVector[i]], [sentenceVector[j]])[0][0]\n",
    "        else: \n",
    "            sentenceCosinSimilarity[i] = cosine_similarity([sentenceVector[i]], [sentenceVector[j]])[0][0]\n",
    "    sentenceCosinSimilarity[i]/=len(sentenceVector[i])\n",
    "sentenceCosinSimilarity = [ [cosin, idx] for idx, cosin in sentenceCosinSimilarity.items() ]\n",
    "sentenceCosinSimilarity.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's the widest release of all time.\n",
      "I'm happy for the filmmakers.\"\n",
      "The movie has great reviews.\n",
      "A UK newspaper called it a \"thrilling\" blockbuster.\n",
      "He is happy he did.\n"
     ]
    }
   ],
   "source": [
    "sim = [idx for sim, idx in sentenceCosinSimilarity[:5]]\n",
    "sim.sort()\n",
    "for idx in sim:\n",
    "    print(tokenize_sentence_raw[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e54217f9e10c9c2ab2c5acb791143dc555f80e81eec2f2e8946a5b329595e68c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
