{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZUzX1xfUqR7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48_jnVrNUqR-"
      },
      "outputs": [],
      "source": [
        "text = open(\"news2.txt\", \"r\").read().replace(\"\\n\\n\", \" \").replace(\"\\n\", \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evbk8EnaUqR-"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    tokenized_sentences = sent_tokenize(text)\n",
        "    tokenized_words = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        tokenized_words_sub = []\n",
        "        for words in word_tokenize(sentence):\n",
        "            tokenized_words_sub.append(words)\n",
        "        tokenized_words.append(tokenized_words_sub)\n",
        "    return (tokenized_words, tokenized_sentences)\n",
        "\n",
        "def removeStopwords(text):\n",
        "    tokenized_words = []\n",
        "    eng = set(stopwords.words(\"english\"))\n",
        "    for sentence in text:\n",
        "        tokenized_words_sub = []\n",
        "        for word in sentence:\n",
        "            if word not in eng:\n",
        "                tokenized_words_sub.append(word)\n",
        "        tokenized_words.append(tokenized_words_sub)\n",
        "    return tokenized_words\n",
        "\n",
        "def removePunch(text):\n",
        "    tokenized_words = []\n",
        "    punch = set(\"\"\"~!@#$%^&*()_+`-=}{|[]\\;'\":<>?,./\"\"\")\n",
        "    for sentence in text:\n",
        "        tokenized_words_sub = []\n",
        "        for word in sentence:\n",
        "            str_word = \"\"\n",
        "            for chr in word:\n",
        "                if chr not in punch:\n",
        "                    str_word+=chr.lower()\n",
        "            if len(str_word) > 1:\n",
        "                tokenized_words_sub.append(str_word)\n",
        "        tokenized_words.append(tokenized_words_sub)\n",
        "    return tokenized_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA8ZHi3UUqR_"
      },
      "outputs": [],
      "source": [
        "tokenized_words, tokenized_sentences = tokenize(text)\n",
        "tokenized_words = removeStopwords(tokenized_words)\n",
        "tokenized_words = removePunch(tokenized_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B94X9vTqUqSA",
        "outputId": "da2533fa-a7a6-4b5d-f162-64cab6260d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['movie', 'film', 'maverick', 'the', 'cruise', 'happy', 'gun', 'best', 'also', 'first', 'it', 'said', 'top', 'showed', 'opening', 'ever', 'tom', 'american', 'broken', 'day', 'memorial', 'drive', 'record', 'broke', 'new', 'time', 'release', 'widest', 'canada', 'north', 'usa', 'cinemas', 'sales', '4700', 'over', 'records', 'biggest', 'history', 'number', 'theatres', 'he', 'johnny', 'beat', 'ridiculously', 'fantastic', 'filmmakers', 'great', 'reviews', 'uk', 'newspaper', 'called', 'thrilling', 'blockbuster', 'another', 'studio', 'action', 'years', 'at', 'want', 'overthetop', 'results', 'make', 'these', 'depp', '2007', 'pirates', 'caribbean', 'gave', '100', 'million', 'weekend', 'sequel', '1986', 'production', 'company', 'done', 'well', 'spokesman', 'my']\n"
          ]
        }
      ],
      "source": [
        "vec = Word2Vec(tokenized_words, window=1, min_count=1, vector_size=100, epochs=100, sg=1)\n",
        "print(vec.wv.index_to_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-SbB4P2UqSA"
      },
      "source": [
        "<h3>Sentence vector</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bs-Y6BAUqSC",
        "outputId": "a56a4d8d-bcd4-4666-aa50-12acaefb0be1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_vector = []\n",
        "for sentence in tokenized_words:\n",
        "    sum = 0\n",
        "    for word in sentence:\n",
        "        sum+=vec.wv[word]\n",
        "    sentence_vector.append(sum/len(sentence))\n",
        "len(sentence_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upQushbPUqSD"
      },
      "source": [
        "<h3>Cosin similarity</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW4NZL0PUqSD"
      },
      "outputs": [],
      "source": [
        "cosineSimilarityVecotr = {}\n",
        "for i in range(len(sentence_vector)):\n",
        "    for j in range(len(sentence_vector)):\n",
        "        if i in cosineSimilarityVecotr:\n",
        "            cosineSimilarityVecotr[i]+=cosine_similarity([sentence_vector[i]], [sentence_vector[j]])[0][0]\n",
        "        else:\n",
        "            cosineSimilarityVecotr[i]=cosine_similarity([sentence_vector[i]], [sentence_vector[j]])[0][0]\n",
        "    cosineSimilarityVecotr[i]/=len(sentence_vector[i])\n",
        "cosineSimilarityVecotr = [ [idx, sim] for idx, sim in  cosineSimilarityVecotr.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGW874kXUqSE",
        "outputId": "632f14d2-96c0-45e7-c7a3-ca215aaad105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Over 4,700 cinemas in the USA and Canada showed it.\n",
            "It's the widest release of all time.\n",
            "I'm happy for the filmmakers.\"\n",
            "A UK newspaper called it a \"thrilling\" blockbuster.\n",
            "He is happy he did.\n"
          ]
        }
      ],
      "source": [
        "limit = 5\n",
        "def comparator(x): return x[1]\n",
        "cosineSimilarityVecotr.sort(key=comparator)\n",
        "cosineSimilarityVecotr = cosineSimilarityVecotr[:5]\n",
        "cosineSimilarityVecotr.sort()\n",
        "for idx, sim in cosineSimilarityVecotr:\n",
        "    print(tokenized_sentences[idx])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e54217f9e10c9c2ab2c5acb791143dc555f80e81eec2f2e8946a5b329595e68c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
