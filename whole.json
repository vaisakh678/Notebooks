{
    "preprocessing": "\ndef tokenize(text):\n    tokenized_sentences = sent_tokenize(text)\n    tokenized_words = []\n    for sentence in tokenized_sentences:\n        tokenized_words_sub = []\n        for words in word_tokenize(sentence):\n            tokenized_words_sub.append(words)\n        tokenized_words.append(tokenized_words_sub)\n    return (tokenized_words, tokenized_sentences)\n\ndef removeStopwords(text):\n    tokenized_words = []\n    eng = set(stopwords.words(\"english\"))\n    for sentence in text:\n        tokenized_words_sub = []\n        for word in sentence:\n            if word not in eng:\n                tokenized_words_sub.append(word)\n        tokenized_words.append(tokenized_words_sub)\n    return tokenized_words\n\ndef removePunch(text):\n    tokenized_words = []\n    punch = set('~!@#$%^&*()_+`-=}{|[]\\;\":<>?,./')\n    for sentence in text:\n        tokenized_words_sub = []\n        for word in sentence:\n            str_word = \"\"\n            for chr in word:\n                if chr not in punch:\n                    str_word+=chr.lower()\n            if len(str_word) > 1:\n                tokenized_words_sub.append(str_word)\n        tokenized_words.append(tokenized_words_sub)\n    return tokenized_words\n\n",
    "digit-cnn": [
        "sr/bin/env python\n# coding: utf-8",
        "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten, Dropout",
        "mnist = keras.datasets.mnist\n#from keras.datasets import mnist",
        "(x_train_full, y_train_full),(x_test, y_test) = mnist.load_data()\nprint(x_train_full.shape)",
        "print(\"x_train_full\", x_train_full.shape)\nprint(\"y_train_full\", y_train_full.shape)\nprint(\"x_test\", x_test.shape)\nprint(\"y_test\", y_test.shape)",
        "#show first train and test image\nplt.imshow(x_train_full[0])\nprint(y_train_full[0])",
        "plt.imshow(x_test[0])\nprint(y_test[0])",
        "#find the number of train data and test data?\nlen(x_train_full)",
        "len(y_train_full)",
        "len(x_test)",
        "len(y_test)",
        "#print the first train data\nprint(x_train_full[0])",
        "#find dimension of train and test data\nprint(x_train_full.shape)\nprint(y_train_full.shape)\nprint(x_test.shape)\nprint(y_test.shape)",
        "#normalising\nx_train_norm = x_train_full/255.\nx_test_norm = x_test/255.\n#print(x_train_norm[0])\n#print(len(x_train_norm[0]))\nprint(x_train_norm.shape)\nprint(x_test_norm.shape)",
        "print(x_train_norm[0])",
        "X_train = x_train_norm.reshape(-1,28,28,1)   \nX_test = x_test_norm.reshape(-1,28,28,1)\nprint(X_train.shape)\nprint(X_test.shape)\n#-1 no of datas\n#28 pixels row and cols\n# 1 channels\n#cnn in 4 dimension",
        "y_train_full[0]",
        "# Find the unique numbers from the train labels\nclasses = np.unique(y_train_full)\nnClasses = len(classes)\nprint('Total number of outputs : ', nClasses)\nprint('Output classes : ', classes)",
        "x_valid, x_train = X_train[:5000], X_train[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n#",
        "# Building model\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1))) #kernal(filter_intializer for activation)\nmodel.add(Dropout(0.25)) #25% of neurons are removed from the model\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform')) #filter(3,3)\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten()) # flatting the output from the intput layer\nmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))#100 - no of neurons ,classification layer \nmodel.add(Dense(10, activation='softmax'))",
        "model.summary() #32 times filter convaluted",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"sgd\",\n              metrics=[\"accuracy\"]) #sgd gradient descent algo",
        "model_history = model.fit(x_train,y_train,epochs=3,validation_data=(x_valid,y_valid),batch_size=20) # batch size 20 img",
        "# plot loss\nprint(model_history.history.keys())\nhistory_dict= model_history.history\nplt.subplot(2, 1, 1)\nplt.title('Cross Entropy Loss')\nplt.plot(history_dict['loss'], color='blue', label='train')\nplt.plot(history_dict['val_loss'], color='orange', label='test')\n# plot accuracy\nplt.subplot(2, 1, 2)\nplt.title('Classification Accuracy')\nplt.plot(history_dict['accuracy'], color='blue', label='train')\nplt.plot(history_dict['val_accuracy'], color='orange', label='test')\nplt.show()",
        "model.evaluate(x_test, y_test)",
        "from sklearn.model_selection import KFold\nkfold = KFold(5, shuffle=True, random_state=1)\n\t# enumerate splits\nfor train_ix, test_ix in kfold.split(x_train):\n  #define model\n  model = Sequential()\n  model.add(Conv2D(32,(3, 3),activation='relu',kernel_initializer='he_uniform',input_shape=(28, 28, 1)))\n  model.add(MaxPooling2D((2, 2)))\n  model.add(Flatten())\n  model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n  model.add(Dense(10, activation='softmax'))\n  model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"sgd\",metrics=[\"accuracy\"])\n\t# select rows for train and test\n  trainX, trainY, testX, testY = x_train[train_ix], y_train[train_ix], x_train[test_ix], y_train[test_ix]\n\t# fit model\n  history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n\t# evaluate model\n  _, acc = model.evaluate(testX, testY, verbose=0)\n  print('> %.3f' % (acc * 100.0))"
    ],
    "sentiment": [
        "sr/bin/env python\n# coding: utf-8",
        "from keras_preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.metrics import accuracy\nfrom keras import layers\nfrom keras.models import Sequential\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, SimpleRNN",
        "filepath = \"/Users/vaisakh/programs/Notebooks/dataset/Labelled Yelp Dataset.csv\"\ndf = pd.read_csv(filepath, names=[\"Review\", \"label\"], sep=\"\\t\")\ndf",
        "review = df[\"Review\"].values\ny = df[\"label\"].values\nreview_train, review_test, y_train, y_test = train_test_split(\n    review, y, test_size=0.25, random_state=1000)",
        "vectorizer = CountVectorizer()\nvectorizer.fit(review_train)\n\nX_train = vectorizer.transform(review_train)\nX_test = vectorizer.transform(review_test)\nX_train",
        "X_train.shape",
        "# Model1\n\n\ninput_dim = X_train.shape[1]\n\nmodel1 = Sequential()\nmodel1.add(layers.Dense(10, input_dim=input_dim, activation=\"relu\"))\nmodel1.add(layers.Dense(1, activation=\"sigmoid\"))",
        "model1.compile(loss=\"binary_crossentropy\",\n               optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel1.summary()",
        "history = model1.fit(X_train, y_train,\n                     epochs=2,\n\n                     validation_data=(X_test, y_test),\n                     batch_size=10)",
        "loss, accuracy = model1.evaluate(X_train, y_train)\nprint(\"Training Accuracy : \", format(accuracy))\n\nloss, accuracy = model1.evaluate(X_test, y_test)\nprint(\"Test Accuracy : \", format(accuracy))",
        "tokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(review_train)\n\nX_train = tokenizer.texts_to_sequences(review_train)\nX_test = tokenizer.texts_to_sequences(review_test)\n\nvocab_size = len(tokenizer.word_index) + 1\nprint(review_train[500])\nprint(X_train[500])\nprint(vocab_size)",
        "maxlen = 100\n\nX_train = pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\nX_test = pad_sequences(X_test, padding=\"post\", maxlen=maxlen)",
        "# Model 2\n\nembedding_dem = 50\n\nmodel2 = Sequential()\nmodel2.add(layers.Embedding(input_dim=vocab_size,\n                            output_dim=embedding_dem,\n                            input_length=maxlen))\nmodel2.add(layers.Flatten())\nmodel2.add(layers.Dense(10,  activation=\"relu\"))\nmodel2.add(layers.Dense(1, activation=\"sigmoid\"))\n\n\nmodel2.compile(loss=\"binary_crossentropy\",\n               optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel2.summary()",
        "history = model2.fit(X_train, y_train,\n                     epochs=2,\n                     validation_data=(X_test, y_test),\n                     batch_size=10)",
        "loss, accuracy = model2.evaluate(X_train, y_train)\nprint(\"Training Accuracy : \", format(accuracy))\n\nloss, accuracy = model2.evaluate(X_test, y_test)\nprint(\"Test Accuracy : \", format(accuracy))",
        "# Model 3\n\nembedding_dem = 100\n\nmodel3 = Sequential()\nmodel3.add(layers.Embedding(vocab_size, embedding_dem, input_length=maxlen))\nmodel3.add(layers.Conv1D(32, 3, activation=\"relu\"))\nmodel3.add(layers.GlobalMaxPooling1D())\nmodel3.add(layers.Dense(10,  activation=\"relu\"))\nmodel3.add(layers.Dense(1, activation=\"sigmoid\"))\n\n\nmodel3.compile(loss=\"binary_crossentropy\",\n               optimizer=\"adam\", metrics=[\"accuracy\"])",
        "history = model3.fit(X_train, y_train,\n                     epochs=50,\n                     verbose=False,\n                     validation_data=(X_test, y_test),\n                     batch_size=10)",
        "loss, accuracy = model3.evaluate(X_train, y_train, verbose=False,)\nprint(\"Training Accuracy : \", format(accuracy))\n\nloss, accuracy = model3.evaluate(X_test, y_test, verbose=False,)\nprint(\"Test Accuracy : \", format(accuracy))",
        "# 32, 3 = 81.59\n# 32, 5 = 80\n# 32, 7 = 80.40\n# 64, 3 = 80.80\n# 64, 5 = 80\n# 64, 7 = 79.19\n# 128, 3 = 79.60\n# 128, 5 = 78.39\n# 128, 7 = 80.80",
        "# Model 4\n\nembedding_dem = 100\n\nmodel4 = Sequential()\nmodel4.add(layers.Embedding(vocab_size, embedding_dem, input_length=maxlen))\nmodel4.add(layers.SimpleRNN(128, return_sequences=True))\n\nmodel4.add(layers.SimpleRNN(128))\n\nmodel4.add(layers.Dense(10, activation=\"relu\"))\n\nmodel4.add(layers.Dense(1, activation=\"softmax\"))\n\n\nmodel4.compile(loss=\"binary_crossentropy\",\n               optimizer=\"adam\", metrics=[\"accuracy\"])",
        "history = model4.fit(X_train, y_train,\n                     epochs=10,\n                     verbose=False,\n                     validation_data=(X_test, y_test),\n                     batch_size=10)",
        "loss, accuracy = model4.evaluate(X_train, y_train, verbose=False,)\nprint(\"Training Accuracy : \", format(accuracy))\n\nloss, accuracy = model4.evaluate(X_test, y_test, verbose=False,)\nprint(\"Test Accuracy : \", format(accuracy))",
        "# model5 LSTM\n\n\nembedding_dem = 100\n\nmodel5 = Sequential()\nmodel5.add(layers.Embedding(vocab_size, embedding_dem, input_length=maxlen))\nmodel5.add(layers.LSTM(128, return_sequences=True))\nmodel5.add(layers.Dropout(0.2))\nmodel5.add(layers.LSTM(128))\n\nmodel5.add(layers.Dense(10, activation=\"relu\"))\n\nmodel5.add(layers.Dense(1, activation=\"softmax\"))\n\nmodel5.compile(loss=\"binary_crossentropy\",\n               optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel5.summary()",
        "history = model5.fit(X_train, y_train,\n                     epochs=10,\n\n                     validation_data=(X_test, y_test),\n                     batch_size=10)",
        "loss, accuracy = model5.evaluate(X_train, y_train, verbose=False,)\nprint(\"Training Accuracy : \", format(accuracy))\n\nloss, accuracy = model5.evaluate(X_test, y_test, verbose=False,)\nprint(\"Test Accuracy : \", format(accuracy))",
        ""
    ]
}