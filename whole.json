{
    "preprocessing": "\ndef tokenize(text):\n    tokenized_sentences = sent_tokenize(text)\n    tokenized_words = []\n    for sentence in tokenized_sentences:\n        tokenized_words_sub = []\n        for words in word_tokenize(sentence):\n            tokenized_words_sub.append(words)\n        tokenized_words.append(tokenized_words_sub)\n    return (tokenized_words, tokenized_sentences)\n\ndef removeStopwords(text):\n    tokenized_words = []\n    eng = set(stopwords.words(\"english\"))\n    for sentence in text:\n        tokenized_words_sub = []\n        for word in sentence:\n            if word not in eng:\n                tokenized_words_sub.append(word)\n        tokenized_words.append(tokenized_words_sub)\n    return tokenized_words\n\ndef removePunch(text):\n    tokenized_words = []\n    punch = set('~!@#$%^&*()_+`-=}{|[]\\;\":<>?,./')\n    for sentence in text:\n        tokenized_words_sub = []\n        for word in sentence:\n            str_word = \"\"\n            for chr in word:\n                if chr not in punch:\n                    str_word+=chr.lower()\n            if len(str_word) > 1:\n                tokenized_words_sub.append(str_word)\n        tokenized_words.append(tokenized_words_sub)\n    return tokenized_words\n\n",
    "digit-cnn": [
        "sr/bin/env python\n# coding: utf-8",
        "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten, Dropout",
        "mnist = keras.datasets.mnist\n#from keras.datasets import mnist",
        "(x_train_full, y_train_full),(x_test, y_test) = mnist.load_data()\nprint(x_train_full.shape)",
        "print(\"x_train_full\", x_train_full.shape)\nprint(\"y_train_full\", y_train_full.shape)\nprint(\"x_test\", x_test.shape)\nprint(\"y_test\", y_test.shape)",
        "#show first train and test image\nplt.imshow(x_train_full[0])\nprint(y_train_full[0])",
        "plt.imshow(x_test[0])\nprint(y_test[0])",
        "#find the number of train data and test data?\nlen(x_train_full)",
        "len(y_train_full)",
        "len(x_test)",
        "len(y_test)",
        "#print the first train data\nprint(x_train_full[0])",
        "#find dimension of train and test data\nprint(x_train_full.shape)\nprint(y_train_full.shape)\nprint(x_test.shape)\nprint(y_test.shape)",
        "#normalising\nx_train_norm = x_train_full/255.\nx_test_norm = x_test/255.\n#print(x_train_norm[0])\n#print(len(x_train_norm[0]))\nprint(x_train_norm.shape)\nprint(x_test_norm.shape)",
        "print(x_train_norm[0])",
        "X_train = x_train_norm.reshape(-1,28,28,1)   \nX_test = x_test_norm.reshape(-1,28,28,1)\nprint(X_train.shape)\nprint(X_test.shape)\n#-1 no of datas\n#28 pixels row and cols\n# 1 channels\n#cnn in 4 dimension",
        "y_train_full[0]",
        "# Find the unique numbers from the train labels\nclasses = np.unique(y_train_full)\nnClasses = len(classes)\nprint('Total number of outputs : ', nClasses)\nprint('Output classes : ', classes)",
        "x_valid, x_train = X_train[:5000], X_train[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n#",
        "# Building model\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1))) #kernal(filter_intializer for activation)\nmodel.add(Dropout(0.25)) #25% of neurons are removed from the model\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform')) #filter(3,3)\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten()) # flatting the output from the intput layer\nmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))#100 - no of neurons ,classification layer \nmodel.add(Dense(10, activation='softmax'))",
        "model.summary() #32 times filter convaluted",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"sgd\",\n              metrics=[\"accuracy\"]) #sgd gradient descent algo",
        "model_history = model.fit(x_train,y_train,epochs=3,validation_data=(x_valid,y_valid),batch_size=20) # batch size 20 img",
        "# plot loss\nprint(model_history.history.keys())\nhistory_dict= model_history.history\nplt.subplot(2, 1, 1)\nplt.title('Cross Entropy Loss')\nplt.plot(history_dict['loss'], color='blue', label='train')\nplt.plot(history_dict['val_loss'], color='orange', label='test')\n# plot accuracy\nplt.subplot(2, 1, 2)\nplt.title('Classification Accuracy')\nplt.plot(history_dict['accuracy'], color='blue', label='train')\nplt.plot(history_dict['val_accuracy'], color='orange', label='test')\nplt.show()",
        "model.evaluate(x_test, y_test)",
        "from sklearn.model_selection import KFold\nkfold = KFold(5, shuffle=True, random_state=1)\n\t# enumerate splits\nfor train_ix, test_ix in kfold.split(x_train):\n  #define model\n  model = Sequential()\n  model.add(Conv2D(32,(3, 3),activation='relu',kernel_initializer='he_uniform',input_shape=(28, 28, 1)))\n  model.add(MaxPooling2D((2, 2)))\n  model.add(Flatten())\n  model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n  model.add(Dense(10, activation='softmax'))\n  model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"sgd\",metrics=[\"accuracy\"])\n\t# select rows for train and test\n  trainX, trainY, testX, testY = x_train[train_ix], y_train[train_ix], x_train[test_ix], y_train[test_ix]\n\t# fit model\n  history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n\t# evaluate model\n  _, acc = model.evaluate(testX, testY, verbose=0)\n  print('> %.3f' % (acc * 100.0))"
    ],
    "sentiment": [
        "sr/bin/env python\n# coding: utf-8",
        "from keras_preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.metrics import accuracy\nfrom keras import layers\nfrom keras.models import Sequential\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, SimpleRNN",
        "filepath = \"/Users/vaisakh/programs/Notebooks/dataset/Labelled Yelp Dataset.csv\"\ndf = pd.read_csv(filepath, names=[\"Review\", \"label\"], sep=\"\\t\")\ndf",
        "review = df[\"Review\"].values\ny = df[\"label\"].values\nreview_train, review_test, y_train, y_test = train_test_split(\n    review, y, test_size=0.25, random_state=1000)",
        "vectorizer = CountVectorizer()\nvectorizer.fit(review_train)\n\nX_train = vectorizer.transform(review_train)\nX_test = vectorizer.transform(review_test)\nX_train",
        "X_train.shape",
        "# Model1\n\n\ninput_dim = X_train.shape[1]\n\nmodel1 = Sequential()\nmodel1.add(layers.Dense(10, input_dim=input_dim, activation=\"relu\"))\nmodel1.add(layers.Dense(1, activation=\"sigmoid\"))",
        "model1.compile(loss=\"binary_crossentropy\",\n               optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel1.summary()",
        "history = model1.fit(X_train, y_train,\n                     epochs=2,\n\n                     validation_data=(X_test, y_test),\n                     batch_size=10)",
        "loss, accuracy = model1.evaluate(X_train, y_train)\nprint(\"Training Accuracy : \", format(accuracy))\n\nloss, accuracy = model1.evaluate(X_test, y_test)\nprint(\"Test Accuracy : \", format(accuracy))",
        "tokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(review_train)\n\nX_train = tokenizer.texts_to_sequences(review_train)\nX_test = tokenizer.texts_to_sequences(review_test)\n\nvocab_size = len(tokenizer.word_index) + 1\nprint(review_train[500])\nprint(X_train[500])\nprint(vocab_size)",
        "maxlen = 100\n\nX_train = pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\nX_test = pad_sequences(X_test, padding=\"post\", maxlen=maxlen)",
        "# Model 2\n\nembedding_dem = 50\n\nmodel2 = Sequential()\nmodel2.add(layers.Embedding(input_dim=vocab_size,\n                            output_dim=embedding_dem,\n                            input_length=maxlen))\nmodel2.add(layers.Flatten())\nmodel2.add(layers.Dense(10,  activation=\"relu\"))\nmodel2.add(layers.Dense(1, activation=\"sigmoid\"))\n\n\nmodel2.compile(loss=\"binary_crossentropy\",\n               optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel2.summary()",
        "history = model2.fit(X_train, y_train,\n                     epochs=2,\n                     validation_data=(X_test, y_test),\n                     batch_size=10)",
        "loss, accuracy = model2.evaluate(X_train, y_train)\nprint(\"Training Accuracy : \", format(accuracy))\n\nloss, accuracy = model2.evaluate(X_test, y_test)\nprint(\"Test Accuracy : \", format(accuracy))",
        "# Model 3\n\nembedding_dem = 100\n\nmodel3 = Sequential()\nmodel3.add(layers.Embedding(vocab_size, embedding_dem, input_length=maxlen))\nmodel3.add(layers.Conv1D(32, 3, activation=\"relu\"))\nmodel3.add(layers.GlobalMaxPooling1D())\nmodel3.add(layers.Dense(10,  activation=\"relu\"))\nmodel3.add(layers.Dense(1, activation=\"sigmoid\"))\n\n\nmodel3.compile(loss=\"binary_crossentropy\",\n               optimizer=\"adam\", metrics=[\"accuracy\"])",
        "history = model3.fit(X_train, y_train,\n                     epochs=50,\n                     verbose=False,\n                     validation_data=(X_test, y_test),\n                     batch_size=10)",
        "loss, accuracy = model3.evaluate(X_train, y_train, verbose=False,)\nprint(\"Training Accuracy : \", format(accuracy))\n\nloss, accuracy = model3.evaluate(X_test, y_test, verbose=False,)\nprint(\"Test Accuracy : \", format(accuracy))",
        "# 32, 3 = 81.59\n# 32, 5 = 80\n# 32, 7 = 80.40\n# 64, 3 = 80.80\n# 64, 5 = 80\n# 64, 7 = 79.19\n# 128, 3 = 79.60\n# 128, 5 = 78.39\n# 128, 7 = 80.80",
        "# Model 4\n\nembedding_dem = 100\n\nmodel4 = Sequential()\nmodel4.add(layers.Embedding(vocab_size, embedding_dem, input_length=maxlen))\nmodel4.add(layers.SimpleRNN(128, return_sequences=True))\n\nmodel4.add(layers.SimpleRNN(128))\n\nmodel4.add(layers.Dense(10, activation=\"relu\"))\n\nmodel4.add(layers.Dense(1, activation=\"softmax\"))\n\n\nmodel4.compile(loss=\"binary_crossentropy\",\n               optimizer=\"adam\", metrics=[\"accuracy\"])",
        "history = model4.fit(X_train, y_train,\n                     epochs=10,\n                     verbose=False,\n                     validation_data=(X_test, y_test),\n                     batch_size=10)",
        "loss, accuracy = model4.evaluate(X_train, y_train, verbose=False,)\nprint(\"Training Accuracy : \", format(accuracy))\n\nloss, accuracy = model4.evaluate(X_test, y_test, verbose=False,)\nprint(\"Test Accuracy : \", format(accuracy))",
        "# model5 LSTM\n\n\nembedding_dem = 100\n\nmodel5 = Sequential()\nmodel5.add(layers.Embedding(vocab_size, embedding_dem, input_length=maxlen))\nmodel5.add(layers.LSTM(128, return_sequences=True))\nmodel5.add(layers.Dropout(0.2))\nmodel5.add(layers.LSTM(128))\n\nmodel5.add(layers.Dense(10, activation=\"relu\"))\n\nmodel5.add(layers.Dense(1, activation=\"softmax\"))\n\nmodel5.compile(loss=\"binary_crossentropy\",\n               optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel5.summary()",
        "history = model5.fit(X_train, y_train,\n                     epochs=10,\n\n                     validation_data=(X_test, y_test),\n                     batch_size=10)",
        "loss, accuracy = model5.evaluate(X_train, y_train, verbose=False,)\nprint(\"Training Accuracy : \", format(accuracy))\n\nloss, accuracy = model5.evaluate(X_test, y_test, verbose=False,)\nprint(\"Test Accuracy : \", format(accuracy))",
        ""
    ],
    "summarization": [
        "\"cells\": [\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"0ZUzX1xfUqR7\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"import nltk\\n\",\n        \"from nltk.corpus import stopwords\\n\",\n        \"from gensim.models import Word2Vec\\n\",\n        \"from nltk.tokenize import word_tokenize, sent_tokenize\\n\",\n        \"from sklearn.metrics.pairwise import cosine_similarity\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"48_jnVrNUqR-\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"text = open(\\\"news2.txt\\\", \\\"r\\\").read().replace(\\\"\\\\n\\\\n\\\", \\\" \\\").replace(\\\"\\\\n\\\", \\\" \\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"evbk8EnaUqR-\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"def tokenize(text):\\n\",\n        \"    tokenized_sentences = sent_tokenize(text)\\n\",\n        \"    tokenized_words = []\\n\",\n        \"    for sentence in tokenized_sentences:\\n\",\n        \"        tokenized_words_sub = []\\n\",\n        \"        for words in word_tokenize(sentence):\\n\",\n        \"            tokenized_words_sub.append(words)\\n\",\n        \"        tokenized_words.append(tokenized_words_sub)\\n\",\n        \"    return (tokenized_words, tokenized_sentences)\\n\",\n        \"\\n\",\n        \"def removeStopwords(text):\\n\",\n        \"    tokenized_words = []\\n\",\n        \"    eng = set(stopwords.words(\\\"english\\\"))\\n\",\n        \"    for sentence in text:\\n\",\n        \"        tokenized_words_sub = []\\n\",\n        \"        for word in sentence:\\n\",\n        \"            if word not in eng:\\n\",\n        \"                tokenized_words_sub.append(word)\\n\",\n        \"        tokenized_words.append(tokenized_words_sub)\\n\",\n        \"    return tokenized_words\\n\",\n        \"\\n\",\n        \"def removePunch(text):\\n\",\n        \"    tokenized_words = []\\n\",\n        \"    punch = set(\\\"\\\"\\\"~!@#$%^&*()_+`-=}{|[]\\\\;'\\\":<>?,./\\\"\\\"\\\")\\n\",\n        \"    for sentence in text:\\n\",\n        \"        tokenized_words_sub = []\\n\",\n        \"        for word in sentence:\\n\",\n        \"            str_word = \\\"\\\"\\n\",\n        \"            for chr in word:\\n\",\n        \"                if chr not in punch:\\n\",\n        \"                    str_word+=chr.lower()\\n\",\n        \"            if len(str_word) > 1:\\n\",\n        \"                tokenized_words_sub.append(str_word)\\n\",\n        \"        tokenized_words.append(tokenized_words_sub)\\n\",\n        \"    return tokenized_words\\n\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"EA8ZHi3UUqR_\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"tokenized_words, tokenized_sentences = tokenize(text)\\n\",\n        \"tokenized_words = removeStopwords(tokenized_words)\\n\",\n        \"tokenized_words = removePunch(tokenized_words)\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"B94X9vTqUqSA\",\n        \"outputId\": \"da2533fa-a7a6-4b5d-f162-64cab6260d74\"\n      },\n      \"outputs\": [\n        {\n          \"name\": \"stdout\",\n          \"output_type\": \"stream\",\n          \"text\": [\n            \"['movie', 'film', 'maverick', 'the', 'cruise', 'happy', 'gun', 'best', 'also', 'first', 'it', 'said', 'top', 'showed', 'opening', 'ever', 'tom', 'american', 'broken', 'day', 'memorial', 'drive', 'record', 'broke', 'new', 'time', 'release', 'widest', 'canada', 'north', 'usa', 'cinemas', 'sales', '4700', 'over', 'records', 'biggest', 'history', 'number', 'theatres', 'he', 'johnny', 'beat', 'ridiculously', 'fantastic', 'filmmakers', 'great', 'reviews', 'uk', 'newspaper', 'called', 'thrilling', 'blockbuster', 'another', 'studio', 'action', 'years', 'at', 'want', 'overthetop', 'results', 'make', 'these', 'depp', '2007', 'pirates', 'caribbean', 'gave', '100', 'million', 'weekend', 'sequel', '1986', 'production', 'company', 'done', 'well', 'spokesman', 'my']\\n\"\n          ]\n        }\n      ],\n      \"source\": [\n        \"vec = Word2Vec(tokenized_words, window=1, min_count=1, vector_size=100, epochs=100, sg=1)\\n\",\n        \"print(vec.wv.index_to_key)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"7-SbB4P2UqSA\"\n      },\n      \"source\": [\n        \"<h3>Sentence vector</h3>\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"8Bs-Y6BAUqSC\",\n        \"outputId\": \"a56a4d8d-bcd4-4666-aa50-12acaefb0be1\"\n      },\n      \"outputs\": [\n        {\n          \"data\": {\n            \"text/plain\": [\n              \"16\"\n            ]\n          },\n          \"execution_count\": 108,\n          \"metadata\": {},\n          \"output_type\": \"execute_result\"\n        }\n      ],\n      \"source\": [\n        \"sentence_vector = []\\n\",\n        \"for sentence in tokenized_words:\\n\",\n        \"    sum = 0\\n\",\n        \"    for word in sentence:\\n\",\n        \"        sum+=vec.wv[word]\\n\",\n        \"    sentence_vector.append(sum/len(sentence))\\n\",\n        \"len(sentence_vector)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"upQushbPUqSD\"\n      },\n      \"source\": [\n        \"<h3>Cosin similarity</h3>\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"CW4NZL0PUqSD\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"cosineSimilarityVecotr = {}\\n\",\n        \"for i in range(len(sentence_vector)):\\n\",\n        \"    for j in range(len(sentence_vector)):\\n\",\n        \"        if i in cosineSimilarityVecotr:\\n\",\n        \"            cosineSimilarityVecotr[i]+=cosine_similarity([sentence_vector[i]], [sentence_vector[j]])[0][0]\\n\",\n        \"        else:\\n\",\n        \"            cosineSimilarityVecotr[i]=cosine_similarity([sentence_vector[i]], [sentence_vector[j]])[0][0]\\n\",\n        \"    cosineSimilarityVecotr[i]/=len(sentence_vector[i])\\n\",\n        \"cosineSimilarityVecotr = [ [idx, sim] for idx, sim in  cosineSimilarityVecotr.items()]\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"xGW874kXUqSE\",\n        \"outputId\": \"632f14d2-96c0-45e7-c7a3-ca215aaad105\"\n      },\n      \"outputs\": [\n        {\n          \"name\": \"stdout\",\n          \"output_type\": \"stream\",\n          \"text\": [\n            \"Over 4,700 cinemas in the USA and Canada showed it.\\n\",\n            \"It's the widest release of all time.\\n\",\n            \"I'm happy for the filmmakers.\\\"\\n\",\n            \"A UK newspaper called it a \\\"thrilling\\\" blockbuster.\\n\",\n            \"He is happy he did.\\n\"\n          ]\n        }\n      ],\n      \"source\": [\n        \"limit = 5\\n\",\n        \"def comparator(x): return x[1]\\n\",\n        \"cosineSimilarityVecotr.sort(key=comparator)\\n\",\n        \"cosineSimilarityVecotr = cosineSimilarityVecotr[:5]\\n\",\n        \"cosineSimilarityVecotr.sort()\\n\",\n        \"for idx, sim in cosineSimilarityVecotr:\\n\",\n        \"    print(tokenized_sentences[idx])\"\n      ]\n    }\n  ],\n  \"metadata\": {\n    \"colab\": {\n      \"provenance\": []\n    },\n    \"kernelspec\": {\n      \"display_name\": \"base\",\n      \"language\": \"python\",\n      \"name\": \"python3\"\n    },\n    \"language_info\": {\n      \"codemirror_mode\": {\n        \"name\": \"ipython\",\n        \"version\": 3\n      },\n      \"file_extension\": \".py\",\n      \"mimetype\": \"text/x-python\",\n      \"name\": \"python\",\n      \"nbconvert_exporter\": \"python\",\n      \"pygments_lexer\": \"ipython3\",\n      \"version\": \"3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]\"\n    },\n    \"orig_nbformat\": 4,\n    \"vscode\": {\n      \"interpreter\": {\n        \"hash\": \"e54217f9e10c9c2ab2c5acb791143dc555f80e81eec2f2e8946a5b329595e68c\"\n      }\n    }\n  },\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0\n}"
    ],
    "cifar": [
        "- coding: utf-8 -*-\n\"\"\"NLP CIFAR CNN.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1ebStY-KT_GODIgngDumx9dWlsYUWPW-B\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n\ncifar =  keras.datasets.cifar10\n\n(x_train, y_train),(x_test, y_test) = cifar.load_data()\nprint(x_train.shape)\n\nplt.imshow(x_train[7])\n\nprint(y_test[0])\n\nx_train[0]\n\nx_train[0].shape\n\nx_train_norm = x_train/255\nx_test_norm = x_test/255\n\nx_train_norm.shape\n\nx_train_norm[0]\n\n# Find the unique numbers from the train labels\nclasses = np.unique(y_train)\nnClasses = len(classes)\nprint('Total number of outputs : ', nClasses)\nprint('Output classes : ', classes)\n\nprint(x_train.shape)\nprint(x_test.shape)\n\nfrom tensorflow.keras.utils import to_categorical\ny_cat_train  = to_categorical(y_train, 10)\ny_cat_test = to_categorical(y_test, 10)\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (4,4), input_shape = (32, 32, 3), activation = \"relu\"))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (4,4), input_shape = (32, 32, 3), activation = \"relu\"))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(512, activation = \"relu\"))\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dense(128, activation = \"relu\"))\n\nmodel.add(Dense(10, activation = \"softmax\"))\n\nmodel.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n\nmodel.summary()\n\n\"\"\"Alert: This Will Take very Looooooooooooooooooooong Time to Complete\"\"\"\n\nmodel_history = model.fit(x_train,y_cat_train,epochs=10,validation_data=(x_test,y_cat_test))"
    ],
    "digit-rnn": [
        "sr/bin/env python\n# coding: utf-8",
        "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, SimpleRNN\nimport matplotlib.pyplot as plt",
        "mnist = tf.keras.datasets.mnist\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train1=x_train/255.0\nx_test1=x_test/255.0",
        "print(x_train1.shape)",
        "model=Sequential()\nmodel.add(SimpleRNN(128, input_shape=(28,28),return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(SimpleRNN(128))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(10,activation='softmax'))\n\n\nmodel.summary()",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"sgd\",\n              metrics=[\"accuracy\"])",
        "model_history = model.fit(x_train1,y_train,epochs=3,validation_data=(x_test1,y_test))",
        "scores=model.evaluate(x_test,y_test,verbose=0)\nprint(\"Accuracy: %.2f%%\" %(scores[1]*100))\n# loss, accuracy = model.evaluate(x_test, y_train,verbose=0 )\n# print(\"Training Accuracy : \", format(accuracy))",
        "# plot loss\nprint(model_history.history.keys())\nhistory_dict= model_history.history\nplt.subplot(2, 1, 1)\nplt.title('Cross Entropy Loss')\nplt.plot(history_dict['loss'], color='blue', label='train')\nplt.plot(history_dict['val_loss'], color='orange', label='test')\n# plot accuracy\nplt.subplot(2, 1, 2)\nplt.title('Classification Accuracy')\nplt.plot(history_dict['accuracy'], color='blue', label='train')\nplt.plot(history_dict['val_accuracy'], color='orange', label='test')\nplt.show()",
        "#implementing LSTM\n\nmodel2=Sequential()\nmodel2.add(LSTM(128, input_shape=(28,28),return_sequences=True))\nmodel2.add(Dropout(0.2))\n\nmodel2.add(LSTM(128))\nmodel2.add(Dropout(0.2))\n\nmodel2.add(Dense(32,activation='relu'))\nmodel2.add(Dropout(0.2))\n\nmodel2.add(Dense(10,activation='softmax'))\n\n\nmodel2.summary()",
        "model2.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"sgd\",\n              metrics=[\"accuracy\"])",
        "model2_history = model2.fit(x_train1,y_train,epochs=3,validation_data=(x_test1,y_test))",
        "scores=model2.evaluate(x_train1,y_train,verbose=0)\nprint(\"Accuracy: %.2f%%\" %(scores[1]*100))\nscores\n\n\n# how to load CIFAR10\n# implement cnn in this program\n#"
    ]
}